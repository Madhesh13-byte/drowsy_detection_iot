{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l20hWtIhZpoe",
        "outputId": "50c12d03-0487-4b3e-82a7-c170f062ce9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.12/dist-packages (0.10.21)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.9.23)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.25.8)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.2.1)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (1.16.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install mediapipe opencv-python numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "\n",
        "# Initialize MediaPipe Face Mesh\n",
        "# refine_landmarks=True gives us more detailed landmarks around the eyes and lips\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True)"
      ],
      "metadata": {
        "id": "lJcGuBa2bm1d"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper Functions for Google Colab Video I/O ---\n",
        "\n",
        "def video_stream():\n",
        "  \"\"\"\n",
        "  Starts a video stream in the Colab notebook using JavaScript to access the webcam.\n",
        "  \"\"\"\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var pendingResolve = null;\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return;\n",
        "      }\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<div style='padding-bottom: 5px;'><b>Status: </b><span id='status'></span></div>\";\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.display = 'none';\n",
        "      imgElement.width = div.clientWidth - 6;\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"user\"}});\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = video.videoWidth;\n",
        "      captureCanvas.height = video.videoHeight;\n",
        "      captureCanvas.getContext('2d').drawImage(video, 0, 0);\n",
        "\n",
        "      video.style.display = 'none';\n",
        "      imgElement.style.display = 'block';\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "    }\n",
        "\n",
        "    async function takePhoto(quality) {\n",
        "      await createDom();\n",
        "      captureCanvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      const data = captureCanvas.toDataURL('image/jpeg', quality);\n",
        "      return data;\n",
        "    }\n",
        "\n",
        "    function showFrame(imgData) {\n",
        "        imgElement.src = 'data:image/jpeg;base64,' + imgData;\n",
        "    }\n",
        "\n",
        "    function updateStatus(status) {\n",
        "        document.getElementById('status').innerText = status;\n",
        "    }\n",
        "  ''')\n",
        "  display(js)\n",
        "\n",
        "def take_photo(quality=0.8):\n",
        "  \"\"\"Captures a single frame from the video stream as a Base64 JPEG.\"\"\"\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  return data\n",
        "\n",
        "def show_frame(img_data):\n",
        "    \"\"\"Displays a frame in the Colab output.\"\"\"\n",
        "    eval_js(f'showFrame(\"{img_data}\")')\n",
        "\n",
        "def update_status(status):\n",
        "    \"\"\"Updates the status text in the Colab output.\"\"\"\n",
        "    eval_js(f'updateStatus(\"{status}\")')\n",
        "\n",
        "\n",
        "# --- Core Logic and Image Conversion Functions ---\n",
        "\n",
        "def base64_to_cv2(b64str):\n",
        "    \"\"\"Decodes a Base64 string to an OpenCV image.\"\"\"\n",
        "    data = b64str.split(',')[1]\n",
        "    img_bytes = b64decode(data)\n",
        "    img_arr = np.frombuffer(img_bytes, dtype=np.uint8)\n",
        "    return cv2.imdecode(img_arr, cv2.IMREAD_COLOR)\n",
        "\n",
        "def cv2_to_base64(img):\n",
        "    \"\"\"Encodes an OpenCV image to a Base64 string.\"\"\"\n",
        "    _, buffer = cv2.imencode('.jpg', img)\n",
        "    return b64encode(buffer).decode('utf-8')\n",
        "\n",
        "def get_eye_aspect_ratio(landmarks, eye_indices):\n",
        "    \"\"\"Calculates the Eye Aspect Ratio (EAR) for a single eye.\"\"\"\n",
        "    p1 = landmarks[eye_indices[0]]\n",
        "    p2 = landmarks[eye_indices[1]]\n",
        "    p3 = landmarks[eye_indices[2]]\n",
        "    p4 = landmarks[eye_indices[3]]\n",
        "    p5 = landmarks[eye_indices[4]]\n",
        "    p6 = landmarks[eye_indices[5]]\n",
        "\n",
        "    vertical1 = ((p2.x - p6.x)**2 + (p2.y - p6.y)**2)**0.5\n",
        "    vertical2 = ((p3.x - p5.x)**2 + (p3.y - p5.y)**2)**0.5\n",
        "    horizontal = ((p1.x - p4.x)**2 + (p1.y - p4.y)**2)**0.5\n",
        "\n",
        "    if horizontal == 0:\n",
        "        return 0.0\n",
        "\n",
        "    ear = (vertical1 + vertical2) / (2.0 * horizontal)\n",
        "    return ear\n",
        "def draw_dotted_line(frame, pt1, pt2, color, thickness=1, gap=7):\n",
        "    \"\"\"Draws a dotted line on a frame by plotting circles along a path.\"\"\"\n",
        "    dist = ((pt1[0] - pt2[0])**2 + (pt1[1] - pt2[1])**2)**.5\n",
        "    pts = []\n",
        "    if dist > 0:\n",
        "      for i in np.arange(0, dist, gap):\n",
        "          r = i / dist\n",
        "          x = int((pt1[0] * (1 - r) + pt2[0] * r) + .5)\n",
        "          y = int((pt1[1] * (1 - r) + pt2[1] * r) + .5)\n",
        "          p = (x, y)\n",
        "          pts.append(p)\n",
        "\n",
        "    for p in pts:\n",
        "        cv2.circle(frame, p, thickness, color, -1)\n",
        "print(\"Helper functions defined successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh3as8C-bpPu",
        "outputId": "e9d302d8-12de-47c0-ec88-b203ae9fca66"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper functions defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Application Loop (V3 with EAR Visualization) ---\n",
        "\n",
        "# Constants\n",
        "EYE_AR_THRESH = 0.23         # Threshold for eye closure. Adjust as needed.\n",
        "DROWSY_TIME_SECONDS = 5      # Seconds of consecutive eye closure to trigger an alert.\n",
        "\n",
        "# State variable for the timer\n",
        "drowsy_start_time = None\n",
        "\n",
        "print(\"Starting video stream... Please allow camera access in your browser.\")\n",
        "video_stream()\n",
        "print(\"Video stream started. Running detection loop...\")\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        # Capture a frame\n",
        "        frame_b64 = take_photo()\n",
        "        frame = base64_to_cv2(frame_b64)\n",
        "\n",
        "        # Get frame dimensions for converting normalized landmarks to pixel coordinates\n",
        "        frame_height, frame_width, _ = frame.shape\n",
        "\n",
        "        # Process the frame\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = face_mesh.process(rgb_frame)\n",
        "\n",
        "        status_text = \"Awake\"\n",
        "        display_color = (0, 255, 0) # Green\n",
        "\n",
        "        if results.multi_face_landmarks:\n",
        "            for face_landmarks in results.multi_face_landmarks:\n",
        "                landmarks = face_landmarks.landmark\n",
        "\n",
        "                # --- Define Eye Landmark Indices ---\n",
        "                # These are the 6 points for each eye used in the EAR calculation\n",
        "                left_eye_indices = [33, 160, 158, 133, 153, 144]\n",
        "                right_eye_indices = [362, 385, 387, 263, 373, 380]\n",
        "\n",
        "                # --- Calculate EAR ---\n",
        "                left_ear = get_eye_aspect_ratio(landmarks, left_eye_indices)\n",
        "                right_ear = get_eye_aspect_ratio(landmarks, right_eye_indices)\n",
        "                avg_ear = (left_ear + right_ear) / 2.0\n",
        "\n",
        "                # --- NEW: Visualization Logic ---\n",
        "                # We will draw lines on both eyes\n",
        "                for eye_indices in [left_eye_indices, right_eye_indices]:\n",
        "                    # Get the 6 landmark points for the current eye\n",
        "                    p1 = landmarks[eye_indices[0]]\n",
        "                    p2 = landmarks[eye_indices[1]]\n",
        "                    p3 = landmarks[eye_indices[2]]\n",
        "                    p4 = landmarks[eye_indices[3]]\n",
        "                    p5 = landmarks[eye_indices[4]]\n",
        "                    p6 = landmarks[eye_indices[5]]\n",
        "\n",
        "                    # Convert normalized coordinates to pixel coordinates\n",
        "                    p1_px = (int(p1.x * frame_width), int(p1.y * frame_height))\n",
        "                    p2_px = (int(p2.x * frame_width), int(p2.y * frame_height))\n",
        "                    p3_px = (int(p3.x * frame_width), int(p3.y * frame_height))\n",
        "                    p4_px = (int(p4.x * frame_width), int(p4.y * frame_height))\n",
        "                    p5_px = (int(p5.x * frame_width), int(p5.y * frame_height))\n",
        "                    p6_px = (int(p6.x * frame_width), int(p6.y * frame_height))\n",
        "\n",
        "                    # Draw the lines on the frame\n",
        "                    draw_dotted_line(frame, p2_px, p6_px, (0, 255, 0), thickness=1, gap=7)\n",
        "                    draw_dotted_line(frame, p3_px, p5_px, (0, 255, 0), thickness=1, gap=7)\n",
        "                    draw_dotted_line(frame, p1_px, p4_px, (0, 255, 0), thickness=1, gap=7)\n",
        "                    # Vertical lines in green\n",
        "                    #cv2.line(frame, p2_px, p6_px, (0, 255, 0), 1)\n",
        "                    #cv2.line(frame, p3_px, p5_px, (0, 255, 0), 1)\n",
        "                    # Horizontal line in red\n",
        "                    #cv2.line(frame, p1_px, p4_px, (0, 0, 255), 1)\n",
        "\n",
        "                # --- Display EAR and Check Drowsiness ---\n",
        "                ear_text = f\"EAR: {avg_ear:.2f}\"\n",
        "                cv2.putText(frame, ear_text, (10, 30),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
        "\n",
        "                if avg_ear < EYE_AR_THRESH:\n",
        "                    if drowsy_start_time is None:\n",
        "                        drowsy_start_time = time.time()\n",
        "                    else:\n",
        "                        elapsed_time = time.time() - drowsy_start_time\n",
        "                        # --- NEW: Drowsiness Timer Visualization ---\n",
        "                        bar_x, bar_y, bar_width, bar_height = 50, 120, 300, 25\n",
        "                        fill_ratio = elapsed_time / DROWSY_TIME_SECONDS\n",
        "                        fill_width = min(bar_width, int(fill_ratio * bar_width))\n",
        "\n",
        "                        # Draw background of the bar\n",
        "                        cv2.rectangle(frame, (bar_x, bar_y), (bar_x + bar_width, bar_y + bar_height), (128, 128, 128), -1)\n",
        "                        # Draw the filling part of the bar\n",
        "                        cv2.rectangle(frame, (bar_x, bar_y), (bar_x + fill_width, bar_y + bar_height), (0, 165, 255), -1) # Orange fill\n",
        "                        if elapsed_time >= DROWSY_TIME_SECONDS:\n",
        "                            status_text = \"DROWSY!\"\n",
        "                            display_color = (0, 0, 255)\n",
        "                else:\n",
        "                    drowsy_start_time = None\n",
        "                    status_text = \"Awake\"\n",
        "                    display_color = (0, 255, 0)\n",
        "        else:\n",
        "            status_text = \"No Face Detected\"\n",
        "            display_color = (255, 165, 0)\n",
        "            drowsy_start_time = None\n",
        "\n",
        "        cv2.putText(frame, status_text, (50, 100),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, display_color, 3)\n",
        "        update_status(status_text)\n",
        "\n",
        "        processed_frame_b64 = cv2_to_base64(frame)\n",
        "        show_frame(processed_frame_b64)\n",
        "\n",
        "    except Exception as e:\n",
        "        if \"NotFoundError\" in str(e) or \"Cannot read\" in str(e):\n",
        "             print(\"Camera stream stopped by user.\")\n",
        "             break\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        time.sleep(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "QGKXcYgsbxD4",
        "outputId": "4efe3ff3-4188-4851-9abe-342d3239fea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting video stream... Please allow camera access in your browser.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var pendingResolve = null;\n",
              "\n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return;\n",
              "      }\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<div style='padding-bottom: 5px;'><b>Status: </b><span id='status'></span></div>\";\n",
              "      div.appendChild(modelOut);\n",
              "\n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.display = 'none';\n",
              "      imgElement.width = div.clientWidth - 6;\n",
              "      div.appendChild(imgElement);\n",
              "\n",
              "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"user\"}});\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = video.videoWidth;\n",
              "      captureCanvas.height = video.videoHeight;\n",
              "      captureCanvas.getContext('2d').drawImage(video, 0, 0);\n",
              "\n",
              "      video.style.display = 'none';\n",
              "      imgElement.style.display = 'block';\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "    }\n",
              "\n",
              "    async function takePhoto(quality) {\n",
              "      await createDom();\n",
              "      captureCanvas.getContext('2d').drawImage(video, 0, 0);\n",
              "      const data = captureCanvas.toDataURL('image/jpeg', quality);\n",
              "      return data;\n",
              "    }\n",
              "\n",
              "    function showFrame(imgData) {\n",
              "        imgElement.src = 'data:image/jpeg;base64,' + imgData;\n",
              "    }\n",
              "\n",
              "    function updateStatus(status) {\n",
              "        document.getElementById('status').innerText = status;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video stream started. Running detection loop...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a0Vy5Ijkkl3N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}